# -*- coding: utf-8 -*-
"""car_evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sre-P7Z4pbMPAFxlU-fw-LDDe2GICO-f

### Import Packages
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn import metrics

"""### Read the dataset:

"""

data = pd.read_csv('car.data.csv')
#We can check the first five samples of the data:
data.head(5)

"""### Dimension of the data"""

data.shape

data.columns

"""### Check Missing Values"""

data.isnull().sum()

"""Comment: the dataset does not contain any missing values."""

data['class'].value_counts()

data['class'].value_counts().plot(kind = 'bar')
plt.show()

data['safety'].value_counts().plot(kind = 'bar')
plt.show()

import seaborn as sns
sns.countplot(data['buying'], hue = data['class'])
plt.show()

data.info()

"""### Data Preprocessing

We must separate independent (X) and target (y) variables for further analysis. We drop the target variable “class” to form X and also define class as the target variable.
"""

X = data.drop(['buying'], axis = 1)
y = data['buying']

"""First, we define the categories of each of the variables in ascending order. We will be using a sklearn package for this task- OrdinalEncoder using a given list of categories. For instance, after the conversion, the values of buying_price_category will convert into [0, 1, 2, 3] from [‘low’, ‘med’, ‘high’, ‘vhigh’] and for the variable doors: after the conversion of the categories will be: [‘2’, ‘3’, ‘4’, ‘5more’] → [0, 1, 2, 3]"""

from sklearn.preprocessing import OrdinalEncoder
maint_cost_category = ['low', 'med', 'high', 'vhigh']
doors_category = ['2', '3', '4', '5more']
person_capacity_category = ['2', '4', 'more']
lug_boot_category = ['small', 'med', 'big']
safety_category = ['low', 'med', 'high']
class_category = ['unacc', 'acc', 'good', 'vgood']
all_categories = [maint_cost_category,doors_category,person_capacity_category,lug_boot_category,safety_category, class_category]
oe = OrdinalEncoder(categories= all_categories)
X = oe.fit_transform( data[['maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']])

"""### Train-test data splitting

Dividing the dataset into train and test data where train data will be used for training the model and test data will be used for evaluating the model. Here, we divide the dataset into 70% : 30% for training and test data, respectively.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)

"""### Model Development

#### Train Decision tree classifier

We train the decision tree classifier in sklearn with the X_train and y_train data. Before starting the training process, we can set the value of different hyperparameters such as criterion, max_depth (maximum depth of the tree), .min_samples_split (the minimum number of samples required to split an internal node). etc..
"""

DT_classifier = DecisionTreeClassifier( criterion= 'gini', max_depth= 3, min_samples_split= 5)
DT_classifier.fit(X_train, y_train)

"""### Experimental Results

#### Prediction
"""

y_pred = DT_classifier.predict(X_test)

"""We use a confusion matrix for finding the performance of the model:"""

confusion_matrix(y_test, y_pred)

print(metrics.classification_report(y_test, y_pred))

from sklearn import tree
fig = plt.figure(figsize=(15,12))
_ = tree.plot_tree(DT_classifier,
feature_names=data.columns[:-1],
class_names= DT_classifier.classes_, filled=True)

data = [[3,3,1,3,3,3],[3,3,2,3,3,3],[3,3,3,3,3,3]]
z_test = pd.DataFrame(data)
z_pred = DT_classifier.predict(z_test)

print(z_pred)

